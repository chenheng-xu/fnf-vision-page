<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fourier Neural Filter as Generic Vision Backbone</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Header -->
        <header class="header">
            <h1 class="title">Fourier Neural Filter as<br>Generic Vision Backbone</h1>
            <div class="authors">
                <div class="author-row">
                    <span class="author"><strong>Chenheng Xu</strong><sup>1,2,3,4</sup></span>
                    <span class="author"><strong>Yuanhao Ban</strong><sup>2</sup></span>
                    <span class="author"><strong>Dan Wu</strong><sup>2</sup></span>
                </div>
                <div class="author-row">
                    <span class="author"><strong>Cho-Jui Hsieh</strong><sup>2,‚úâ</sup></span>
                    <span class="author"><strong>Ying Nian Wu</strong><sup>2,‚úâ</sup></span>
                    <span class="author"><strong>Yixin Zhu</strong><sup>1,3,4,‚úâ</sup></span>
                </div>
                <div class="affiliations">
                    <div class="affil-row">
                        <sup>1</sup>Peking University
                        <sup>2</sup>University of California, Los Angeles
                    </div>
                    <div class="affil-row">
                        <sup>3</sup>State Key Lab of General AI, Peking University
                        <sup>4</sup>Beijing Key Laboratory of Behavior and Mental Health, Peking University
                    </div>
                </div>
            </div>
            <div class="links">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" class="btn btn-primary" target="_blank">üìÑ Paper</a>
                <a href="https://github.com/chenheng-xu/fnf-vision-code" class="btn btn-secondary" target="_blank">üíª Code</a>
                <a href="https://chenheng-xu.github.io/fnf-vision-page/" class="btn btn-secondary" target="_blank">üåê Project Page</a>
            </div>
            <div class="badges">
                <span class="badge">ICLR 2026</span>
                <span class="badge">Vision Backbone</span>
                <span class="badge">Fourier Transform</span>
            </div>
        </header>

        <!-- Teaser/Overview -->
        <section class="section teaser-section">
            <div class="teaser-content">
                <div class="teaser-text">
                    <h2>Unified Time-Frequency Vision Backbone</h2>
                    <p class="teaser-description">
                        ViF introduces the first unified backbone that couples time-domain and frequency-domain analysis, 
                        achieving state-of-the-art performance across image classification, object detection, and semantic segmentation 
                        while maintaining competitive computational efficiency.
                    </p>
                    <div class="teaser-stats">
                        <div class="stat-item">
                            <div class="stat-value">85.2%</div>
                            <div class="stat-label">ImageNet Top-1</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-value">50.1</div>
                            <div class="stat-label">COCO Box mAP</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-value">52.3</div>
                            <div class="stat-label">ADE20K mIoU</div>
                        </div>
                    </div>
                </div>
                <div class="teaser-image">
                    <object data="figures/model_schematic.pdf" type="application/pdf" class="teaser-img pdf-object">
                        <p>Your browser does not support PDFs. <a href="figures/model_schematic.pdf">Download the PDF</a>.</p>
                    </object>
                </div>
            </div>
        </section>

        <!-- Abstract -->
        <section class="section">
            <h2>Abstract</h2>
            <p class="abstract">
                Effective information extraction has long been a central challenge in Computer Vision. 
                Transformer- and Mamba-based backbones have significantly advanced this field by providing 
                powerful long-range modeling capability, even though they are initially developed for Natural 
                Language Processing. Recent progress has highlighted the potential of Fourier Neural Operator (FNO), 
                which, with its favorable quasi-linear complexity and strong global modeling capacity, offers a 
                promising alternative for visual representation learning. However, FNO exhibits a fundamental 
                limitation in capturing local high-frequency patterns due to the over-smoothing effect and bandwidth 
                bottleneck.
            </p>
            <p class="abstract">
                To address this limitation, we propose <strong>Vision Filter (ViF)</strong>, as a generic backbone 
                for Computer Vision, consisting of two complementary components: <strong>adaptive modulation</strong> 
                for enhancing sensitivity to high-frequency component in the frequency domain, and 
                <strong>selective activation</strong> for balancing local time-domain and global frequency-domain 
                information flow. Extensive experiments reveal that ViF consistently outperforms prominent variants 
                of Transformer- and Mamba-based backbones across diverse visual tasks, including image classification, 
                object detection, and semantic segmentation. ViF demonstrates lower computational complexity than 
                Transformer-based models and better structural modeling than Mamba-based models, which suffer from 
                spatial disruption due to their directional scanning mechanism.
            </p>
        </section>

        <!-- Key Contributions -->
        <section class="section">
            <h2>Key Contributions</h2>
            <ul class="contributions">
                <li>We propose <strong>Fourier Neural Filter (FNF)</strong>, the first unified backbone that couples 
                    time-domain and frequency-domain analysis, inherently preserving the spatial structure of 2D visual 
                    representation.</li>
                <li>We theoretically and empirically demonstrate that our proposed FNF resolves the inherent 
                    over-smoothing effect and bandwidth bottleneck of the original FNO.</li>
                <li>The proposed model <strong>ViF</strong> achieves state-of-the-art performance on three mainstream 
                    visual tasks: ImageNet-1K classification, COCO object detection, and ADE20K semantic segmentation.</li>
            </ul>
        </section>

        <!-- Method Overview -->
        <section class="section">
            <h2>Method Overview</h2>
            <div class="method-intro">
                <p>
                    Our <strong>Fourier Neural Filter (FNF)</strong> extends the standard FNO by introducing an input-dependent 
                    kernel function that enables selective activation of local time-domain and global frequency-domain information 
                    flow through Hadamard product operations. This design addresses two fundamental limitations of FNO:
                </p>
            </div>
            <div class="method-grid">
                <div class="method-card">
                    <div class="method-icon">üîß</div>
                    <h3>Fourier Neural Filter (FNF)</h3>
                    <p>An input-dependent integral kernel operator that enables adaptive information flow between 
                    time and frequency domains, constructing a unified time-frequency representation space.</p>
                    <div class="method-formula">
                        <code>(K v)(x) = T(G(v) ‚äô P(v))(x)</code>
                    </div>
                </div>
                <div class="method-card">
                    <div class="method-icon">üìä</div>
                    <h3>Adaptive Modulation</h3>
                    <p>Frequency balancing through amplitude-sensitive weighting, attenuating dominant low-frequency 
                    components while enhancing weak high-frequency components.</p>
                    <div class="method-formula">
                        <code>M(z) = z ‚äô [Œ≤ ¬∑ ||z||<sup>Œ±</sup>]</code>
                    </div>
                </div>
                <div class="method-card">
                    <div class="method-icon">‚ö°</div>
                    <h3>Selective Activation</h3>
                    <p>Element-wise multiplication in time domain that achieves joint time-frequency modulation, 
                    enhancing informative mid-/high-frequency components while suppressing redundant ones.</p>
                    <div class="method-formula">
                        <code>F(G(v) ‚äô P(v)) = ƒú(v) * PÃÇ(v)</code>
                    </div>
                </div>
            </div>
            <div class="method-figure">
                <object data="figures/fnf_backbone.pdf" type="application/pdf" class="method-img pdf-object">
                    <p>Your browser does not support PDFs. <a href="figures/fnf_backbone.pdf">Download the PDF</a>.</p>
                </object>
                <p class="figure-caption"><strong>Figure:</strong> Schematic diagram of our proposed FNF backbone.</p>
            </div>
        </section>

        <!-- Results -->
        <section class="section">
            <h2>Results</h2>
            
            <div class="results-grid">
                <div class="result-card">
                    <h3>ImageNet-1K Classification</h3>
                    <div class="metric">
                        <span class="metric-label">ViF-Tiny:</span>
                        <span class="metric-value">83.8% Top-1</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Small:</span>
                        <span class="metric-value">84.5% Top-1</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Base:</span>
                        <span class="metric-value">85.2% Top-1</span>
                    </div>
                    <p class="result-note">Outperforms Transformer- and Mamba-based models with competitive computational efficiency</p>
                </div>

                <div class="result-card">
                    <h3>COCO Object Detection</h3>
                    <div class="metric">
                        <span class="metric-label">ViF-Tiny:</span>
                        <span class="metric-value">47.7 Box mAP</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Small:</span>
                        <span class="metric-value">49.1 Box mAP</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Base:</span>
                        <span class="metric-value">50.1 Box mAP</span>
                    </div>
                    <p class="result-note">State-of-the-art performance on dense prediction tasks</p>
                </div>

                <div class="result-card">
                    <h3>ADE20K Segmentation</h3>
                    <div class="metric">
                        <span class="metric-label">ViF-Tiny:</span>
                        <span class="metric-value">49.6 mIoU</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Small:</span>
                        <span class="metric-value">51.3 mIoU</span>
                    </div>
                    <div class="metric">
                        <span class="metric-label">ViF-Base:</span>
                        <span class="metric-value">52.3 mIoU</span>
                    </div>
                    <p class="result-note">Superior performance with fewer parameters and FLOPs</p>
                </div>
            </div>

            <div class="comparison-visualization">
                <h3>Model Efficiency Comparison</h3>
                <object data="figures/model_comparison.pdf" type="application/pdf" class="comparison-img pdf-object">
                    <p>Your browser does not support PDFs. <a href="figures/model_comparison.pdf">Download the PDF</a>.</p>
                </object>
                <p class="figure-caption"><strong>Figure:</strong> Model efficiency comparison on ImageNet-1k. Throughput tested on H100 GPU with batch size 128 and input resolution 224√ó224.</p>
            </div>

            <div class="comparison-table">
                <h3>Detailed Performance Metrics</h3>
                <p class="table-note">Throughput tested on H100 GPU with batch size 128 and input resolution 224√ó224</p>
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Params (M)</th>
                                <th>FLOPs (G)</th>
                                <th>Top-1 (%)</th>
                                <th>Throughput (im/s)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight">
                                <td><strong>ViF-Tiny</strong></td>
                                <td>29</td>
                                <td>5.1</td>
                                <td><strong>83.8</strong></td>
                                <td>1549</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>ViF-Small</strong></td>
                                <td>45</td>
                                <td>7.8</td>
                                <td><strong>84.5</strong></td>
                                <td>-</td>
                            </tr>
                            <tr class="highlight">
                                <td><strong>ViF-Base</strong></td>
                                <td>96</td>
                                <td>16.7</td>
                                <td><strong>85.2</strong></td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Swin-T</td>
                                <td>28</td>
                                <td>4.5</td>
                                <td>81.3</td>
                                <td>1217</td>
                            </tr>
                            <tr>
                                <td>VMamba-T</td>
                                <td>30</td>
                                <td>4.9</td>
                                <td>82.6</td>
                                <td>1243</td>
                            </tr>
                            <tr>
                                <td>ConvNeXt-T</td>
                                <td>29</td>
                                <td>4.5</td>
                                <td>82.1</td>
                                <td>1286</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>

        <!-- Architecture -->
        <section class="section">
            <h2>Architecture</h2>
            <p>
                Our ViF model is structured into four hierarchical stages, mirroring the design principles of 
                established vision backbones. The architecture consists of:
            </p>
            <ul class="architecture-list">
                <li><strong>Stem Layer:</strong> Overlapped convolution for initial feature extraction</li>
                <li><strong>Four Stages:</strong> Each stage contains multiple ViF blocks with down-sampling layers</li>
                <li><strong>ViF Block:</strong> Comprises FNF module and FFN module with residual connections</li>
                <li><strong>FNF Module:</strong> Two-branch design with local and global convolutions</li>
            </ul>
            <div class="architecture-note">
                <p><strong>Key Innovation:</strong> Unlike FNO which uses a fixed integral kernel, FNF employs an 
                input-dependent kernel that adaptively modulates information flow between time and frequency domains, 
                addressing the over-smoothing effect and bandwidth bottleneck.</p>
            </div>
            <div class="architecture-visualization">
                <object data="figures/model_schematic.pdf" type="application/pdf" class="architecture-img pdf-object">
                    <p>Your browser does not support PDFs. <a href="figures/model_schematic.pdf">Download the PDF</a>.</p>
                </object>
                <p class="figure-caption"><strong>Figure:</strong> Schematic diagram of our proposed ViF architecture. Architecture details can be found in the Appendix.</p>
            </div>
        </section>

        <!-- Effective Receptive Field -->
        <section class="section">
            <h2>Effective Receptive Field Comparison</h2>
            <p class="section-intro">
                We compare the effective receptive field (ERF) characteristics of Transformer, Fourier Neural Filter (FNF), 
                and Mamba with different scanning strategies. FNF leverages Fourier transforms to operate in the frequency domain, 
                enabling global receptive field coverage through spectral convolutions with O(N log N) complexity.
            </p>
            <div class="erf-visualization">
                <img src="figures/erf.png" alt="Effective Receptive Field Comparison" class="erf-img" />
                <p class="figure-caption"><strong>Figure:</strong> Effective Receptive Field Comparison. Transformer and FNF achieve uniform global coverage, 
                while Mamba variants exhibit distance-dependent patterns influenced by their scanning strategies.</p>
            </div>
        </section>

        <!-- Task Visualizations -->
        <section class="section">
            <h2>Task Visualizations</h2>
            <div class="task-visualizations">
                <div class="task-vis-item">
                    <h3>Object Detection on COCO</h3>
                    <img src="figures/det.png" alt="Object Detection Visualization" class="task-img" />
                    <p class="figure-caption">Visualization of object detection results on COCO dataset.</p>
                </div>
                <div class="task-vis-item">
                    <h3>Semantic Segmentation on ADE20K</h3>
                    <img src="figures/seg.png" alt="Semantic Segmentation Visualization" class="task-img" />
                    <p class="figure-caption">Visualization of semantic segmentation results on ADE20K dataset.</p>
                </div>
            </div>
        </section>

        <!-- Spatial and Frequency Analysis -->
        <section class="section">
            <h2>Spatial and Frequency Analysis</h2>
            <p class="section-intro">
                We perform comprehensive spatial- and frequency-domain analyses on the fourth-layer features to quantitatively 
                assess the influence of different modules (SA and AM) on feature representations.
            </p>
            <div class="analysis-visualizations">
                <div class="analysis-grid">
                    <div class="analysis-item">
                        <img src="figures/cls1.png" alt="Spatial and Frequency Analysis 1" class="analysis-img" />
                    </div>
                    <div class="analysis-item">
                        <img src="figures/cls2.png" alt="Spatial and Frequency Analysis 2" class="analysis-img" />
                    </div>
                    <div class="analysis-item">
                        <img src="figures/cls3.png" alt="Spatial and Frequency Analysis 3" class="analysis-img" />
                    </div>
                    <div class="analysis-item">
                        <img src="figures/cls4.png" alt="Spatial and Frequency Analysis 4" class="analysis-img" />
                    </div>
                    <div class="analysis-item">
                        <img src="figures/cls5.png" alt="Spatial and Frequency Analysis 5" class="analysis-img" />
                    </div>
                    <div class="analysis-item">
                        <img src="figures/cls6.png" alt="Spatial and Frequency Analysis 6" class="analysis-img" />
                    </div>
                </div>
                <p class="figure-caption"><strong>Figure:</strong> Visualization of spatial and frequency analysis showing the impact of different modules 
                on feature representations across various input images.</p>
            </div>
        </section>

        <!-- Theoretical Contributions -->
        <section class="section">
            <h2>Theoretical Contributions</h2>
            <div class="theoretical-grid">
                <div class="theoretical-card">
                    <h3>Bandwidth Bottleneck Analysis</h3>
                    <p>We prove that any FNO layer with fixed bandwidth K has an irreducible truncation error 
                    in the frequency domain, limiting its ability to capture high-frequency patterns.</p>
                    <div class="theorem-box">
                        <strong>Proposition 1:</strong> If v is non-bandlimited, the truncation error satisfies:
                        <code>inf<sub>F<sub>K</sub></sub> ||F<sub>K</sub>(v) - T(v)|| ‚â• ||P<sub>K</sub><sup>‚ä•</sup> T(v)||</code>
                    </div>
                </div>
                <div class="theoretical-card">
                    <h3>Over-smoothing Effect</h3>
                    <p>We demonstrate that multiplicative contraction on mid-/high-frequency modes accumulates 
                    exponentially with depth, leading to progressive suppression of high-frequency information.</p>
                    <div class="theorem-box">
                        <strong>Proposition 2:</strong> The overall frequency response |H<sub>L</sub>(k)| ‚â§ œÅ<sup>L</sup> ‚Üí 0 
                        on {|k| ‚â• k<sub>0</sub>} as L ‚Üí ‚àû, causing over-smoothing.
                    </div>
                </div>
                <div class="theoretical-card">
                    <h3>FNF Solution</h3>
                    <p>Our proposed FNF with adaptive modulation and selective activation theoretically resolves 
                    both limitations by enabling non-uniform frequency amplification and adaptive information flow.</p>
                    <div class="theorem-box">
                        <strong>Key Innovation:</strong> Input-dependent kernel Œ∫(x,y;v) enables adaptive modulation 
                        that preserves high-frequency components while maintaining stability.
                    </div>
                </div>
            </div>
        </section>

        <!-- Ablation Study -->
        <section class="section">
            <h2>Ablation Study</h2>
            <p class="section-intro">
                We conduct comprehensive ablation studies to validate the effectiveness of each component in our ViF model.
            </p>
            <div class="ablation-table">
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th>Model Variant</th>
                                <th>Top-1 (%)</th>
                                <th>Params (M)</th>
                                <th>FLOPs (G)</th>
                                <th>Throughput</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight">
                                <td><strong>ViF-T (Full)</strong></td>
                                <td><strong>83.8</strong></td>
                                <td>29</td>
                                <td>5.1</td>
                                <td>1549</td>
                            </tr>
                            <tr>
                                <td>w/o LC-1</td>
                                <td>83.6</td>
                                <td>28</td>
                                <td>5.0</td>
                                <td>1585</td>
                            </tr>
                            <tr>
                                <td>w/o LC-2</td>
                                <td>83.4</td>
                                <td>28</td>
                                <td>5.0</td>
                                <td>1589</td>
                            </tr>
                            <tr>
                                <td>w/o Adaptive Modulation</td>
                                <td>83.5</td>
                                <td>29</td>
                                <td>5.1</td>
                                <td>1667</td>
                            </tr>
                            <tr>
                                <td>w/o Selective Activation</td>
                                <td>83.1</td>
                                <td>25</td>
                                <td>4.6</td>
                                <td>1689</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <div class="ablation-note">
                <p><strong>Key Findings:</strong> Selective Activation (SA) has the largest impact on model performance, 
                with accuracy dropping to 83.1% when removed. Both local convolutions (LC-1 and LC-2) contribute 
                significantly to the model's effectiveness.</p>
            </div>
            <div class="ablation-visualization">
                <h3>Visualization of Ablation Study</h3>
                <div class="ablation-images">
                    <div class="ablation-image-item">
                        <object data="figures/input.pdf" type="application/pdf" class="ablation-img pdf-object">
                            <p><a href="figures/input.pdf">Input</a></p>
                        </object>
                        <p class="image-label">(a) Input</p>
                    </div>
                    <div class="ablation-image-item">
                        <object data="figures/am.pdf" type="application/pdf" class="ablation-img pdf-object">
                            <p><a href="figures/am.pdf">w/o AM</a></p>
                        </object>
                        <p class="image-label">(b) w/o AM</p>
                    </div>
                    <div class="ablation-image-item">
                        <object data="figures/sa.pdf" type="application/pdf" class="ablation-img pdf-object">
                            <p><a href="figures/sa.pdf">w/o SA</a></p>
                        </object>
                        <p class="image-label">(c) w/o SA</p>
                    </div>
                    <div class="ablation-image-item">
                        <object data="figures/full.pdf" type="application/pdf" class="ablation-img pdf-object">
                            <p><a href="figures/full.pdf">ViF</a></p>
                        </object>
                        <p class="image-label">(d) ViF</p>
                    </div>
                </div>
                <p class="figure-caption"><strong>Figure:</strong> Visualization of ablation study for adaptive modulation (AM) and selective activation (SA).</p>
            </div>
        </section>

        <!-- Quick Start -->
        <section class="section quick-start">
            <h2>Quick Start</h2>
            <div class="quick-start-content">
                <div class="code-block">
                    <h3>Installation</h3>
                    <pre><code># Clone the repository
git clone https://github.com/chenheng-xu/fnf-vision-code.git
cd fnf-vision-code

# Install dependencies
pip install -r requirements.txt</code></pre>
                </div>
                <div class="code-block">
                    <h3>Usage Example</h3>
                    <pre><code>from fnf_vision import ViF

# Initialize ViF model
model = ViF(model_size='tiny')  # or 'small', 'base'

# Load pretrained weights
model.load_pretrained('vif_tiny.pth')

# Forward pass
output = model(input_image)</code></pre>
                </div>
            </div>
            <div class="quick-start-links">
                <a href="https://github.com/chenheng-xu/fnf-vision-code" class="btn btn-secondary" target="_blank">
                    üìö View Full Documentation
                </a>
                <a href="https://github.com/chenheng-xu/fnf-vision-code#installation" class="btn btn-secondary" target="_blank">
                    üöÄ Get Started
                </a>
            </div>
        </section>

        <!-- Citation -->
        <section class="section">
            <h2>Citation</h2>
            <div class="citation-box">
                <pre><code>@article{xu2025fourier,
  title={Fourier Neural Filter as Generic Vision Backbone},
  author={Xu, Chenheng and Ban, Yuanhao and Wu, Dan and Hsieh, Cho-Jui and Wu, Ying Nian and Zhu, Yixin},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}</code></pre>
            </div>
        </section>

        <!-- News/Updates -->
        <section class="section news-section">
            <h2>News & Updates</h2>
            <ul class="news-list">
                <li>
                    <span class="news-date">Dec 2025</span>
                    <span class="news-content">Paper submitted to ICLR 2026</span>
                </li>
                <li>
                    <span class="news-date">Dec 2025</span>
                    <span class="news-content">Code repository released on GitHub</span>
                </li>
                <li>
                    <span class="news-date">Dec 2025</span>
                    <span class="news-content">Project webpage launched</span>
                </li>
            </ul>
        </section>

        <!-- Footer -->
        <footer class="footer">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>Contact</h4>
                    <p>For questions and inquiries, please contact:</p>
                    <ul class="contact-list">
                        <li>Cho-Jui Hsieh: <a href="mailto:chohsieh@ucla.edu">chohsieh@ucla.edu</a></li>
                        <li>Ying Nian Wu: <a href="mailto:ywu@stat.ucla.edu">ywu@stat.ucla.edu</a></li>
                        <li>Yixin Zhu: <a href="mailto:yixin.zhu@pku.edu.cn">yixin.zhu@pku.edu.cn</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h4>Acknowledgments</h4>
                    <p>This work is supported in part by the National Natural Science Foundation of China, 
                    the PKU-BingJi Joint Laboratory for Artificial Intelligence, and the National Comprehensive 
                    Experimental Base for Governance of Intelligent Society.</p>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 The Authors. All rights reserved.</p>
                <p class="footer-note">This work is submitted to ICLR 2026.</p>
            </div>
        </footer>
    </div>
</body>
</html>

